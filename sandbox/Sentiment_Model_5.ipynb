{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Sentiment Predicting Model on a Social Media Corpus\n",
    "\n",
    "### Using the 1.6 Million Tweets Positive/Negative Sentiment Corpus\n",
    "\n",
    "\n",
    "Helpful Resources:\n",
    "https://medium.com/@thoszymkowiak/how-to-implement-sentiment-analysis-using-word-embedding-and-convolutional-neural-networks-on-keras-163197aef623\n",
    "\n",
    "Good Post I found that most of this code is built off of\n",
    "https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html\n",
    "\n",
    "Corpus (1.6million tweets as positive/negative)\n",
    "https://drive.google.com/uc?id=0B04GJPshIjmPRnZManQwWEdTZjg&export=download\n",
    "http://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import scipy.optimize\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Helper libraries (from w266 Materials).\n",
    "# import segment\n",
    "#from shared_lib import utils\n",
    "from shared_lib import vocabulary\n",
    "\n",
    "# Machine Learning Packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Word2Vec Model\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Conv1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (1621665, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dear @Microsoft the newOoffice for Mac is grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Microsoft how about you make a system that do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>@MikeWolf1980 @Microsoft I will be downgrading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@Microsoft 2nd computer with same error!!! #Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>After attempting a reinstall, it still bricks,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0  dear @Microsoft the newOoffice for Mac is grea...\n",
       "1          0  @Microsoft how about you make a system that do...\n",
       "6          0  @MikeWolf1980 @Microsoft I will be downgrading...\n",
       "7          0  @Microsoft 2nd computer with same error!!! #Wi...\n",
       "9          0  After attempting a reinstall, it still bricks,..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull in Tweet Data (Must be downloaded using https://github.com/seirasto/twitter_download)\n",
    "\n",
    "# tweets\n",
    "\n",
    "cols = ['TweetID', 'Sentiment', 'SentimentText']\n",
    "tweets = pd.read_table(\"Data/twitter_download-master/ALL_SEMEVAL_TRAIN_DATA.txt\", header=None,\n",
    "                       names=cols, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "# num_tweets = 1000000\n",
    "# tweets = pd.read_csv('Data/milliontweetCorpus/training.1600000.processed.noemoticon.csv', \n",
    "#                      header=None, names=cols, encoding='ISO-8859-1', error_bad_lines=False) #, nrows=num_tweets)\n",
    "tweets.drop(['TweetID'], axis=1, inplace=True)\n",
    "tweets = tweets[tweets.Sentiment.isnull() == False]\n",
    "tweets['Sentiment'] = tweets['Sentiment'].map({'negative':0, 'neutral':2, 'positive':4})\n",
    "\n",
    "cols2 = ['Sentiment', 'ItemID', 'DateTime', 'Query', 'SentimentSource', 'SentimentText']\n",
    "tweets2 = pd.read_csv('Data/milliontweetCorpus/training.1600000.processed.noemoticon.csv', \n",
    "                     header=None, names=cols2, encoding='ISO-8859-1', error_bad_lines=False) #, nrows=num_tweets)\n",
    "tweets2.drop(['ItemID', 'DateTime', 'Query', 'SentimentSource'], axis=1, inplace=True)\n",
    "tweets2 = tweets2[tweets2.Sentiment.isnull() == False]\n",
    "tweets2['Sentiment'] = tweets2['Sentiment'].map(int)\n",
    "\n",
    "tweets = pd.concat([tweets,tweets2], axis=0)\n",
    "\n",
    "tweets = tweets[tweets['SentimentText'].isnull() == False]\n",
    "tweets.reset_index(inplace=True)\n",
    "tweets.drop('index', axis=1, inplace=True)\n",
    "print('dataset loaded with shape', tweets.shape)\n",
    "tweets = tweets[tweets['SentimentText'] != 'Not Available']\n",
    "tweets[tweets['Sentiment'] == 0].head(5)\n",
    "# SemEval Dataset is actually relatively small (6000 tweets in 2016). \n",
    "# We can group all of the Train/Test/Dev data from 2013 through 2016 to get more.\n",
    "# Additionally, we could consider using this data which has 1.6 million rows but it is only a binary positive/negative class \n",
    "# https://drive.google.com/uc?id=0B04GJPshIjmPRnZManQwWEdTZjg&export=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Tweet: dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\n",
      "Negative Tweet: @Microsoft how about you make a system that doesn't eat my friggin discs. This is the 2nd time this has happened and I am so sick of it!\n",
      "Negative Tweet: @MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail\n",
      "Negative Tweet: @Microsoft 2nd computer with same error!!! #Windows10fail Guess we will shelve this until SP1! http://t.co/QCcHlKuy8Q\n",
      "Negative Tweet: After attempting a reinstall, it still bricks, says, \"Windows cannot finish installing,\" or somesuch. @Microsoft may have cost me $600.\n",
      "Negative Tweet: Did @Microsoft break Windows 10? Was working fine on Wednesday but now I can't get passed the login screen without it freezing up.\n",
      "Negative Tweet: For the 1st time @Skype has a \"High Startup impact\"   Does anyone at @Microsoft have a clue? #Windows10Fail http://t.co/loO3yd5rwe\n",
      "Negative Tweet: #teens @BillGates 1st company failed miserably. When Gates &amp; @PaulGAllen tried to sell the product it wouldn't work #nevergiveup @Microsoft\n",
      "Negative Tweet: @Microsoft support for 365 has been terrible. On the phone for an hour then got dropped. So far not worth the headache. May ask for refund.\n",
      "Negative Tweet: Hey @Microsoft, I reserved my copy of @Windows 10 back in June, but i haven't been invited to download. Any updates on the timeline?\n",
      "Negative Tweet: @eyesonfoxorg @Microsoft I'm still using Vista on one &amp; Win-7 on another, Vista is a dinosaur, unfortunately I may use a free 10 with limits\n",
      "Negative Tweet: @Microsoft Is it normal that it takes hours to check the email name for creating an microsoft account? 2nd try!\n",
      "Negative Tweet: After 75 minutes of being on hold with @Microsoft in India 1-800-936-5700 \"Adrian\" wants to transfer my call again (3rd time) #Windows10Fail\n",
      "Negative Tweet: @Microsoft On hold with support for 52 minutes now. C'mon.\n",
      "Negative Tweet: Beyond frustrated with my #Xbox360 right now, and that as of June, @Microsoft doesn't support it. Gotta find someone else to fix the drive.\n",
      "Negative Tweet: @Microsoft Heard you are a software company. Why then is most of your software so bad that it has to be replaced by 3rd party apps?\n",
      "Negative Tweet: @Microsoft @Windows Daily windows updates suck! W/ all the $$$ and drones U have working 4 U, maybe U guys could get it right the 1st time?\n",
      "Negative Tweet: Call from \"John\" @Microsoft \"hello, i'd like to look at your computer, over the phone\" Me:\" ok, may I ask why your calling about my..(1/2)\n",
      "Negative Tweet: I thought @Microsoft was retiring the Lumia line to give way to Surface Mobile? I guess not. https://t.co/7NRWeBqjEN\n",
      "Negative Tweet: @microsoft using Office 2013's Bing dictionary. type in \"bound.\" This is the 3rd picture they show me. WTF?... http://t.co/VzwGIoLxco\n",
      "Negative Tweet: I've just been told for the 5th time that my case number \"will be escalated\" to a Level II tech by @MicrosoftHelps @Microsoft @Windows\n",
      "Negative Tweet: Now I'm trying to get to @MicrosoftHelps @Microsoft @Windows support online for about the 5th time.m\n",
      "Negative Tweet: Spending the day on the phone with @Microsoft. This is not how I wanted to spend my Sunday\n",
      "Negative Tweet: @Microsoft @MicrosoftHelps For the 2nd time I have been charged for something I wasnt made aware of. Person on the phone didn't care.\n",
      "Negative Tweet: 3rd rep just hung up on me. Absolutely devastated at how @microsoft's customer service treated me, wow.\n",
      "Negative Tweet: @Microsoft may you PLEASE release versions of Windows when its stable Windows 10 is no where near stable!\n",
      "Negative Tweet: no diff from @Microsoft with BitLocker =at some point, platforms should provide data at rest encryption built in https://t.co/KATCGJER4W\n",
      "Negative Tweet: Why does @Microsoft make it so hard to reset my @skype password? RIDICULOUS. Still not sure why password stopped working in the 1st place!\n",
      "Negative Tweet: And @Microsoft's Mail, Calendar, Music, and Video apps have been rewritten for the third time in four years... https://t.co/Rn27JuMGMP\n",
      "Negative Tweet: @microsoft ur company will give me my 500 pounds plus the cost of the laptop on Monday for what u did to my laptop!\n",
      "Negative Tweet: Hey @Microsoft, fuck you. I don't have Internet until Thursday and my Xbox one won't even let me sign onto my account offline.\n",
      "Negative Tweet: @ @Microsoft  3rd time my Lumia handset submitted in service Centre in last 8 Months, No one is looking into it. Need replacement of my 930\n",
      "Negative Tweet: @Lumia #Lumia @Microsoft 2nd, you guys haven't released a lumia that has a QHD screen, or takes video in 2k resolution yet.\n",
      "Negative Tweet: Why would anyone want to form a Metallica cover band? What is the upside-one day if they are lucky they may open for an AC/DC cover band?\n",
      "Negative Tweet: Red Sox game I have tickets to tonight may get rained out, same with AC/DC concert tomorrow night. Somebody give me some good news...\n",
      "Negative Tweet: Promised to take my niece to AC/DC tomorrow.  Soooo not in the mood now.\n",
      "Negative Tweet: My grandma \"despises\" me going to see an AC/DC concert tomorrow\n",
      "Negative Tweet: Guy I'm working with probably disappointed that I don't care he's going to see AC/DC tomorrow\n",
      "Negative Tweet: AC/DC is in Toronto tomorrow and I can't believe I'm gonna miss it\n",
      "Negative Tweet: A little worried about Amazon. The collection was meant to go up on Saturday but is still not available.\n",
      "Negative Tweet: E-books in US: prices rise due to deal Big 5 publishers with Amazon, sales down 9.3% (Jan-Jun 15) http://t.co/iaqoUrslrR /v @michaelbhaskar\n",
      "Negative Tweet: May all the items saved for later in your Amazon basket increase in price #moderncurses\n",
      "Negative Tweet: ordered my back pack off of Amazon 3 weeks ago and it turns out they lost my package and school starts tomorrow #sick\n",
      "Negative Tweet: Well one $99 Amazon prime fiasco later my stuff for the game Friday is guaranteed to be here Thursday...I'm counting on u Amazon!\n",
      "Negative Tweet: Amazon's refusal to integrate nicely with the Android ecosystem may have been its undoing in the Fire phone market.  http://t.co/6mBZz0eazc\n",
      "Negative Tweet: Author: IF I've EVER reviewed your book, grab a screen cap NOW! Amazon may pull it down. http://t.co/jA1RTUZlBp http://t.co/hs0Lox27ez\n",
      "Negative Tweet: My Amazon package was supposed to be delivered today by 8pm. Guess that means I'll get it tomorrow before 8pm?\n",
      "Negative Tweet: Damnit Amazon! When you say you're going to ship something on the 8th, I expect it to ship on the 8th!\n",
      "Negative Tweet: @AmazonHelp You may please check my pending complaint with Amazon for which I write many emails and called customer care.\n",
      "Negative Tweet: @Monomirror JELLY. Mine won't be here until Friday because Amazon are tight with shipping dates :(.\n",
      "Negative Tweet: @VofEscaflowne this is the exact reason why I don't preorder Friday releases from Amazon. They always push it back to the Monday delivery.\n",
      "Positive Tweet: Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft\n",
      "Positive Tweet: Just ordered my 1st ever tablet; @Microsoft Surface Pro 3, i7/8GB 512GB SSD. Hopefully it works out for dev to replace my laptop =)\n",
      "Positive Tweet: Sunday morning, quiet day so time to welcome in #Windows10 @Microsoft @Windows http://t.co/7VtvAzhWmV\n",
      "Positive Tweet: Innovation for jobs is just around the corner - to be exact next Wednesday 8/19 at @Microsoft http://t.co/3DK6ToZeA8 http://t.co/L2wOZcwgRb\n",
      "Positive Tweet: #Vote for @AIESEC to become the 10th Global non profit partner of @Microsoft for us to #UpgradeYourWorld together. @AIESECGermany\n",
      "Positive Tweet: Top 5 most searched for Back-to-School topics -- the list may surprise you http://t.co/Xj21uMVo0p  @bing @MSFTnews #backtoschool @Microsoft\n",
      "Positive Tweet: @trucker_squigz @Microsoft @MISpeedway @nationwide88 Cant wait to see you up here. I will be at the race on sun\n",
      "Positive Tweet: @ScottArbeit @GabeAul @Microsoft isntall the newest version and you may chance your mind!\n",
      "Positive Tweet: @taehongmin1 We have an IOT workshop by @Microsoft at 11PM on the Friday - definitely worth going for inspiration! #HackThePlanet\n",
      "Positive Tweet: PAX Prime Thursday is overloaded for me with @Microsoft and Nintendo indie events going down. Also, cider!!! :p\n",
      "Positive Tweet: I traveled to Redmond today. I'm visiting with @Microsoft @SQLServer engineers tomorrow - at their invitation. Feeling excited.\n",
      "Positive Tweet: Have you heard the news? Our next meetup is on September 10th with Ryan Berry of @Microsoft! RSVP now for free -&gt; http://t.co/QOp26oct4f\n",
      "Positive Tweet: We're excited to learn about #cloud #analytics from @Microsoft tomorrow! Join us https://t.co/p0bMREBBHC #tech #rva http://t.co/1XHmPdSvzq\n",
      "Positive Tweet: Pet adoptions are $10 this Friday @BFAS_LA thanks to @Microsoft @Windows. #Upgradeyourworld with a new best friend! http://t.co/0aI2QiBbCR\n",
      "Positive Tweet: http://t.co/luX5VvBrmJ   Register 4 the NACR Skype for Business event with @Microsoft for Sept 16th Chevy Chase, MD #skype4B #contactcenter\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 Negative Tweets (encoded as 0's)\n",
    "for i,text in enumerate(tweets[tweets['Sentiment'] == 0]['SentimentText']):\n",
    "    print('Negative Tweet:', text)\n",
    "    if i >= 50:\n",
    "        break\n",
    "        \n",
    "# Print first 5 Positive Tweets (encoded as 4's)\n",
    "for i,text in enumerate(tweets[tweets['Sentiment'] == 4]['SentimentText']):\n",
    "    print('Positive Tweet:', text)\n",
    "    if i >= 14:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good idea on using the tokenizer.  we can use this as a function with df.apply to speed this up! Check out the stack overflow solution below for some inspiration.  Some exploratory code is below\n",
    "\n",
    "https://stackoverflow.com/questions/33098040/how-to-use-word-tokenize-in-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "t = TweetTokenizer()\n",
    "\n",
    "\n",
    "def create_tokens(tweet):\n",
    "    tweet = str(tweet.lower())\n",
    "    tokens = t.tokenize(tweet)\n",
    "    tokens = list(filter(lambda x: not x.startswith('@'), tokens)) ##\n",
    "    tokens = list(filter(lambda x: not x.startswith('#'), tokens)) ##\n",
    "    tokens = list(filter(lambda x: not x.startswith('http'), tokens)) ##\n",
    "    return tokens\n",
    "\n",
    "tweets['SentimentTextTokenized'] = tweets['SentimentText'].apply(create_tokens)\n",
    "tweets.head()\n",
    "\n",
    "\n",
    "X = tweets.SentimentTextTokenized\n",
    "Y = tweets.Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14e95eaba20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "num_bins = 5\n",
    "n, bins, patches = plt.hist(tweets.Sentiment, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Looks like there are only two categories in this Training Dataset. Negative = 0, Positive = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & (future) Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1423397    [just, did, a, test, drive, with, the, bmw, 11...\n",
      "1595566    [good, luck, lauren, ., i, know, you, are, goi...\n",
      "1431862    [shipley's, donuts, sounds, amazing, right, no...\n",
      "716158     [i, dont, like, it, at, all, ., its, confusing...\n",
      "1581706                          [i, got, it, ..., hehehehe]\n",
      "59613                    [sorry, to, hear, about, your, dog]\n",
      "7657       [no, someone, on, the, phone, when, we, report...\n",
      "979271     [ok, ., done, with, meals, and, all, hangi, ki...\n",
      "89987      [well, there, goes, my, banner, right, down, t...\n",
      "1402181    [and, we, love, you, haha, come, back, to, aus...\n",
      "Name: SentimentTextTokenized, dtype: object\n",
      "1423397    4\n",
      "1595566    4\n",
      "1431862    4\n",
      "716158     0\n",
      "1581706    4\n",
      "59613      0\n",
      "7657       2\n",
      "979271     4\n",
      "89987      0\n",
      "1402181    4\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.20,random_state=100)\n",
    "print(X_train[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14002210"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each word into a vector representation. Couldn't get Keras working with straight indexes for each word so I followed the steps laid out here:\n",
    "# https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html\n",
    "\n",
    "vec_dim = 100\n",
    "\n",
    "tweet_w2v = Word2Vec(size=vec_dim, min_count=5) #vector size and minimum threshold to include for rare words\n",
    "tweet_w2v.build_vocab(x for x in X_train)\n",
    "tweet_w2v.train((x for x in X_train), total_examples=tweet_w2v.corpus_count, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.7805217504501343),\n",
       " ('nice', 0.6860557794570923),\n",
       " ('bad', 0.6596325039863586),\n",
       " ('cool', 0.6542390584945679),\n",
       " ('fantastic', 0.640545129776001),\n",
       " ('tough', 0.6350492835044861),\n",
       " ('weird', 0.625077486038208),\n",
       " ('terrible', 0.6227144002914429),\n",
       " ('rough', 0.6202049851417542),\n",
       " ('g0od', 0.619382381439209)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Vectors \n",
    "https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 31437\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, vec_dim) for z in map(lambda x: x, X_train)])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_vecs_w2v)\n",
    "train_vecs_w2v = scaler.transform(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, vec_dim) for z in map(lambda x: x, X_test)])\n",
    "test_vecs_w2v = scaler.transform(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Y: 1423397    4\n",
      "1595566    4\n",
      "1431862    4\n",
      "716158     0\n",
      "1581706    4\n",
      "59613      0\n",
      "7657       2\n",
      "979271     4\n",
      "89987      0\n",
      "1402181    4\n",
      "Name: Sentiment, dtype: int64\n",
      "Encoded Y: [2 2 2 0 2 0 1 2 0 2]\n",
      "One Hot Y: [[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# This section only needed if doing more than binary classification (multi-class)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Original Y:\", y_train[:10])\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test= encoder.transform(y_test)\n",
    "print(\"Encoded Y:\", y_train[:10])\n",
    "\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_test = to_categorical(y_test, 3)\n",
    "print(\"One Hot Y:\", y_train[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model (really simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_8 to have 3 dimensions, but got array with shape (1293333, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-535b356c61a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vecs_w2v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1555\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1556\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1411\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1413\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1414\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1415\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    140\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                                  str(array.shape))\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_8 to have 3 dimensions, but got array with shape (1293333, 3)"
     ]
    }
   ],
   "source": [
    "# Pad the sequence to the same length (this hurt accuracy?)\n",
    "# train_vecs_w2v = sequence.pad_sequences(train_vecs_w2v, maxlen=vec_dim)\n",
    "# test_vecs_w2v = sequence.pad_sequences(test_vecs_w2v, maxlen=vec_dim)\n",
    "\n",
    "# Build Keras Model\n",
    "\n",
    "# # assemble the embedding_weights in one numpy array\n",
    "# # vocab_dim = 300 # dimensionality of your word vectors\n",
    "# n_symbols = len(index_dict) + 1 # adding 1 to account for 0th index (for masking)\n",
    "# embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "# for word,index in index_dict.items():\n",
    "#     embedding_weights[index, :] = word_vectors[word]\n",
    "\n",
    "# # define inputs here\n",
    "# embedding_layer = Embedding(output_dim=vocab_dim, input_dim=n_symbols, trainable=True)\n",
    "# embedding_layer.build((None,)) # if you don't do this, the next step won't work\n",
    "# embedding_layer.set_weights([embedding_weights])\n",
    "\n",
    "# embedded = embedding_layer(input_layer)\n",
    "\n",
    "# max_review_length = 30\n",
    "# X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tfidf), vec_dim))\n",
    "model.add(Conv1D(32, 3, padding='same'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax')) # softmax if multi-class\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy', # categorical_crossentropy if multi-class\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.add(Embedding(top_words, vec_dim, input_length=max_review_length))\n",
    "# model.add(Conv1D(64, 3, padding='same'))\n",
    "# model.add(Conv1D(32, 3, border_mode='same'))\n",
    "# model.add(Conv1D(16, 3, border_mode='same'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(180,activation='sigmoid'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1,activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to Predict Positive/Neutral/Negative\n",
    "\n",
    "def prediction(text):\n",
    "    sentiment = [\"Negative\",'Neutral', \"Positive\"]\n",
    "    tokens = create_tokens(text)\n",
    "    vecs = buildWordVector(tokens, vec_dim)\n",
    "    vecs = scaler.transform(vecs)\n",
    "    predic = model.predict(vecs)\n",
    "    result = sentiment[predic.argmax(axis=1)[0]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "#this is a read-only instance\n",
    "reddit = praw.Reddit(user_agent='first_scrape (by /u/dswald)',\n",
    "                     client_id='TyAK1zSuAvQjmA', \n",
    "                     client_secret=\"uxHGsL0zNODbowN6umVnBWpqLAQ\")\n",
    "\n",
    "subreddit = reddit.subreddit('tensorflow')\n",
    "hot_python = subreddit.hot(limit = 3) #need to view >2 to get past promoted posts\n",
    "\n",
    "for submission in hot_python:\n",
    "    if not submission.stickied: #top 2 are promoted posts, labeled as 'stickied'\n",
    "        print('Title: {}, ups: {}, downs: {}, Have we visited: {}'.format(submission.title,\n",
    "                                                                          submission.ups,\n",
    "                                                                          submission.downs,\n",
    "                                                                          submission.visited))\n",
    "        comments = submission.comments.list() #unstructured\n",
    "        for comment in comments:\n",
    "            print (20*'-')\n",
    "            print ('Parent ID:', comment.parent())\n",
    "            print ('Comment ID:', comment.id)\n",
    "            print (comment.body)\n",
    "            print(\"#\"*10,'PREDICTED SENTIMENT:', prediction(comment.body),\"#\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that everythng is positive or neutral.  Is this because of a unigram model or too specific training data or what?  This is food for thought.  Committing now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
