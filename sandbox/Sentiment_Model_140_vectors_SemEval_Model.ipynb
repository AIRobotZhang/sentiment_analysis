{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Sentiment Predicting Model on a Social Media Corpus\n",
    "\n",
    "### Using the 1.6 Million Tweets Positive/Negative Sentiment Corpus\n",
    "\n",
    "\n",
    "Helpful Resources:\n",
    "https://medium.com/@thoszymkowiak/how-to-implement-sentiment-analysis-using-word-embedding-and-convolutional-neural-networks-on-keras-163197aef623\n",
    "\n",
    "Good Post I found that most of this code is built off of\n",
    "https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html\n",
    "\n",
    "Corpus (1.6million tweets as positive/negative)\n",
    "https://drive.google.com/uc?id=0B04GJPshIjmPRnZManQwWEdTZjg&export=download\n",
    "http://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import scipy.optimize\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Helper libraries (from w266 Materials).\n",
    "# import segment\n",
    "#from shared_lib import utils\n",
    "from shared_lib import vocabulary\n",
    "\n",
    "# Machine Learning Packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Word2Vec Model\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Conv1D, Flatten, Dropout, Conv2D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (1621665, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dear @Microsoft the newOoffice for Mac is grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Microsoft how about you make a system that do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>@MikeWolf1980 @Microsoft I will be downgrading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@Microsoft 2nd computer with same error!!! #Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>After attempting a reinstall, it still bricks,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0  dear @Microsoft the newOoffice for Mac is grea...\n",
       "1          0  @Microsoft how about you make a system that do...\n",
       "6          0  @MikeWolf1980 @Microsoft I will be downgrading...\n",
       "7          0  @Microsoft 2nd computer with same error!!! #Wi...\n",
       "9          0  After attempting a reinstall, it still bricks,..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Sentiment140 Corpus (to use to encode vectors)\n",
    "\n",
    "cols = ['TweetID', 'Sentiment', 'SentimentText']\n",
    "tweets = pd.read_table(\"Data/twitter_download-master/ALL_SEMEVAL_TRAIN_DATA.txt\", header=None,\n",
    "                       names=cols, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "# num_tweets = 1000000\n",
    "# tweets = pd.read_csv('Data/milliontweetCorpus/training.1600000.processed.noemoticon.csv', \n",
    "#                      header=None, names=cols, encoding='ISO-8859-1', error_bad_lines=False) #, nrows=num_tweets)\n",
    "tweets.drop(['TweetID'], axis=1, inplace=True)\n",
    "tweets = tweets[tweets.Sentiment.isnull() == False]\n",
    "tweets['Sentiment'] = tweets['Sentiment'].map({'negative':0, 'neutral':2, 'positive':4})\n",
    "\n",
    "cols2 = ['Sentiment', 'ItemID', 'DateTime', 'Query', 'SentimentSource', 'SentimentText']\n",
    "tweets2 = pd.read_csv('Data/milliontweetCorpus/training.1600000.processed.noemoticon.csv', \n",
    "                     header=None, names=cols2, encoding='ISO-8859-1', error_bad_lines=False) #, nrows=num_tweets)\n",
    "tweets2.drop(['ItemID', 'DateTime', 'Query', 'SentimentSource'], axis=1, inplace=True)\n",
    "tweets2 = tweets2[tweets2.Sentiment.isnull() == False]\n",
    "tweets2['Sentiment'] = tweets2['Sentiment'].map(int)\n",
    "\n",
    "tweets = pd.concat([tweets,tweets2], axis=0)\n",
    "\n",
    "tweets = tweets[tweets['SentimentText'].isnull() == False]\n",
    "tweets.reset_index(inplace=True)\n",
    "tweets.drop('index', axis=1, inplace=True)\n",
    "print('dataset loaded with shape', tweets.shape)\n",
    "tweets = tweets[tweets['SentimentText'] != 'Not Available']\n",
    "tweets[tweets['Sentiment'] == 0].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (21665, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dear @Microsoft the newOoffice for Mac is grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Microsoft how about you make a system that do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>@MikeWolf1980 @Microsoft I will be downgrading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@Microsoft 2nd computer with same error!!! #Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>After attempting a reinstall, it still bricks,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0  dear @Microsoft the newOoffice for Mac is grea...\n",
       "1          0  @Microsoft how about you make a system that do...\n",
       "6          0  @MikeWolf1980 @Microsoft I will be downgrading...\n",
       "7          0  @Microsoft 2nd computer with same error!!! #Wi...\n",
       "9          0  After attempting a reinstall, it still bricks,..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load SemEval Dataset (to use to train model, better context of a negative/positive tweet than Sentiment140)\n",
    "\n",
    "cols = ['TweetID', 'Sentiment', 'SentimentText']\n",
    "tweets = pd.read_table(\"Data/twitter_download-master/ALL_SEMEVAL_TRAIN_DATA.txt\", header=None,\n",
    "                       names=cols, encoding='ISO-8859-1', error_bad_lines=False)\n",
    "tweets.drop(['TweetID'], axis=1, inplace=True)\n",
    "tweets = tweets[tweets.Sentiment.isnull() == False]\n",
    "tweets['Sentiment'] = tweets['Sentiment'].map({'negative':0, 'neutral':2, 'positive':4})\n",
    "\n",
    "tweets = tweets[tweets['SentimentText'].isnull() == False]\n",
    "tweets.reset_index(inplace=True)\n",
    "tweets.drop('index', axis=1, inplace=True)\n",
    "print('dataset loaded with shape', tweets.shape)\n",
    "tweets = tweets[tweets['SentimentText'] != 'Not Available']\n",
    "tweets[tweets['Sentiment'] == 0].head(5)\n",
    "\n",
    "# SemEval Dataset is actually relatively small (6000 tweets in 2016). \n",
    "# We can group all of the Train/Test/Dev data from 2013 through 2016 to get more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Tweet: dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\n",
      "Negative Tweet: @Microsoft how about you make a system that doesn't eat my friggin discs. This is the 2nd time this has happened and I am so sick of it!\n",
      "Negative Tweet: @MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail\n",
      "Negative Tweet: @Microsoft 2nd computer with same error!!! #Windows10fail Guess we will shelve this until SP1! http://t.co/QCcHlKuy8Q\n",
      "Negative Tweet: After attempting a reinstall, it still bricks, says, \"Windows cannot finish installing,\" or somesuch. @Microsoft may have cost me $600.\n",
      "Negative Tweet: Did @Microsoft break Windows 10? Was working fine on Wednesday but now I can't get passed the login screen without it freezing up.\n",
      "Negative Tweet: For the 1st time @Skype has a \"High Startup impact\"   Does anyone at @Microsoft have a clue? #Windows10Fail http://t.co/loO3yd5rwe\n",
      "Negative Tweet: #teens @BillGates 1st company failed miserably. When Gates &amp; @PaulGAllen tried to sell the product it wouldn't work #nevergiveup @Microsoft\n",
      "Negative Tweet: @Microsoft support for 365 has been terrible. On the phone for an hour then got dropped. So far not worth the headache. May ask for refund.\n",
      "Negative Tweet: Hey @Microsoft, I reserved my copy of @Windows 10 back in June, but i haven't been invited to download. Any updates on the timeline?\n",
      "Negative Tweet: @eyesonfoxorg @Microsoft I'm still using Vista on one &amp; Win-7 on another, Vista is a dinosaur, unfortunately I may use a free 10 with limits\n",
      "Negative Tweet: @Microsoft Is it normal that it takes hours to check the email name for creating an microsoft account? 2nd try!\n",
      "Negative Tweet: After 75 minutes of being on hold with @Microsoft in India 1-800-936-5700 \"Adrian\" wants to transfer my call again (3rd time) #Windows10Fail\n",
      "Negative Tweet: @Microsoft On hold with support for 52 minutes now. C'mon.\n",
      "Negative Tweet: Beyond frustrated with my #Xbox360 right now, and that as of June, @Microsoft doesn't support it. Gotta find someone else to fix the drive.\n",
      "Negative Tweet: @Microsoft Heard you are a software company. Why then is most of your software so bad that it has to be replaced by 3rd party apps?\n",
      "Negative Tweet: @Microsoft @Windows Daily windows updates suck! W/ all the $$$ and drones U have working 4 U, maybe U guys could get it right the 1st time?\n",
      "Negative Tweet: Call from \"John\" @Microsoft \"hello, i'd like to look at your computer, over the phone\" Me:\" ok, may I ask why your calling about my..(1/2)\n",
      "Negative Tweet: I thought @Microsoft was retiring the Lumia line to give way to Surface Mobile? I guess not. https://t.co/7NRWeBqjEN\n",
      "Negative Tweet: @microsoft using Office 2013's Bing dictionary. type in \"bound.\" This is the 3rd picture they show me. WTF?... http://t.co/VzwGIoLxco\n",
      "Negative Tweet: I've just been told for the 5th time that my case number \"will be escalated\" to a Level II tech by @MicrosoftHelps @Microsoft @Windows\n",
      "Negative Tweet: Now I'm trying to get to @MicrosoftHelps @Microsoft @Windows support online for about the 5th time.m\n",
      "Negative Tweet: Spending the day on the phone with @Microsoft. This is not how I wanted to spend my Sunday\n",
      "Negative Tweet: @Microsoft @MicrosoftHelps For the 2nd time I have been charged for something I wasnt made aware of. Person on the phone didn't care.\n",
      "Negative Tweet: 3rd rep just hung up on me. Absolutely devastated at how @microsoft's customer service treated me, wow.\n",
      "Negative Tweet: @Microsoft may you PLEASE release versions of Windows when its stable Windows 10 is no where near stable!\n",
      "Negative Tweet: no diff from @Microsoft with BitLocker =at some point, platforms should provide data at rest encryption built in https://t.co/KATCGJER4W\n",
      "Negative Tweet: Why does @Microsoft make it so hard to reset my @skype password? RIDICULOUS. Still not sure why password stopped working in the 1st place!\n",
      "Negative Tweet: And @Microsoft's Mail, Calendar, Music, and Video apps have been rewritten for the third time in four years... https://t.co/Rn27JuMGMP\n",
      "Negative Tweet: @microsoft ur company will give me my 500 pounds plus the cost of the laptop on Monday for what u did to my laptop!\n",
      "Negative Tweet: Hey @Microsoft, fuck you. I don't have Internet until Thursday and my Xbox one won't even let me sign onto my account offline.\n",
      "Negative Tweet: @ @Microsoft  3rd time my Lumia handset submitted in service Centre in last 8 Months, No one is looking into it. Need replacement of my 930\n",
      "Negative Tweet: @Lumia #Lumia @Microsoft 2nd, you guys haven't released a lumia that has a QHD screen, or takes video in 2k resolution yet.\n",
      "Negative Tweet: Why would anyone want to form a Metallica cover band? What is the upside-one day if they are lucky they may open for an AC/DC cover band?\n",
      "Negative Tweet: Red Sox game I have tickets to tonight may get rained out, same with AC/DC concert tomorrow night. Somebody give me some good news...\n",
      "Negative Tweet: Promised to take my niece to AC/DC tomorrow.  Soooo not in the mood now.\n",
      "Negative Tweet: My grandma \"despises\" me going to see an AC/DC concert tomorrow\n",
      "Negative Tweet: Guy I'm working with probably disappointed that I don't care he's going to see AC/DC tomorrow\n",
      "Negative Tweet: AC/DC is in Toronto tomorrow and I can't believe I'm gonna miss it\n",
      "Negative Tweet: A little worried about Amazon. The collection was meant to go up on Saturday but is still not available.\n",
      "Negative Tweet: E-books in US: prices rise due to deal Big 5 publishers with Amazon, sales down 9.3% (Jan-Jun 15) http://t.co/iaqoUrslrR /v @michaelbhaskar\n",
      "Negative Tweet: May all the items saved for later in your Amazon basket increase in price #moderncurses\n",
      "Negative Tweet: ordered my back pack off of Amazon 3 weeks ago and it turns out they lost my package and school starts tomorrow #sick\n",
      "Negative Tweet: Well one $99 Amazon prime fiasco later my stuff for the game Friday is guaranteed to be here Thursday...I'm counting on u Amazon!\n",
      "Negative Tweet: Amazon's refusal to integrate nicely with the Android ecosystem may have been its undoing in the Fire phone market.  http://t.co/6mBZz0eazc\n",
      "Negative Tweet: Author: IF I've EVER reviewed your book, grab a screen cap NOW! Amazon may pull it down. http://t.co/jA1RTUZlBp http://t.co/hs0Lox27ez\n",
      "Negative Tweet: My Amazon package was supposed to be delivered today by 8pm. Guess that means I'll get it tomorrow before 8pm?\n",
      "Negative Tweet: Damnit Amazon! When you say you're going to ship something on the 8th, I expect it to ship on the 8th!\n",
      "Negative Tweet: @AmazonHelp You may please check my pending complaint with Amazon for which I write many emails and called customer care.\n",
      "Negative Tweet: @Monomirror JELLY. Mine won't be here until Friday because Amazon are tight with shipping dates :(.\n",
      "Negative Tweet: @VofEscaflowne this is the exact reason why I don't preorder Friday releases from Amazon. They always push it back to the Monday delivery.\n",
      "Positive Tweet: Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft\n",
      "Positive Tweet: Just ordered my 1st ever tablet; @Microsoft Surface Pro 3, i7/8GB 512GB SSD. Hopefully it works out for dev to replace my laptop =)\n",
      "Positive Tweet: Sunday morning, quiet day so time to welcome in #Windows10 @Microsoft @Windows http://t.co/7VtvAzhWmV\n",
      "Positive Tweet: Innovation for jobs is just around the corner - to be exact next Wednesday 8/19 at @Microsoft http://t.co/3DK6ToZeA8 http://t.co/L2wOZcwgRb\n",
      "Positive Tweet: #Vote for @AIESEC to become the 10th Global non profit partner of @Microsoft for us to #UpgradeYourWorld together. @AIESECGermany\n",
      "Positive Tweet: Top 5 most searched for Back-to-School topics -- the list may surprise you http://t.co/Xj21uMVo0p  @bing @MSFTnews #backtoschool @Microsoft\n",
      "Positive Tweet: @trucker_squigz @Microsoft @MISpeedway @nationwide88 Cant wait to see you up here. I will be at the race on sun\n",
      "Positive Tweet: @ScottArbeit @GabeAul @Microsoft isntall the newest version and you may chance your mind!\n",
      "Positive Tweet: @taehongmin1 We have an IOT workshop by @Microsoft at 11PM on the Friday - definitely worth going for inspiration! #HackThePlanet\n",
      "Positive Tweet: PAX Prime Thursday is overloaded for me with @Microsoft and Nintendo indie events going down. Also, cider!!! :p\n",
      "Positive Tweet: I traveled to Redmond today. I'm visiting with @Microsoft @SQLServer engineers tomorrow - at their invitation. Feeling excited.\n",
      "Positive Tweet: Have you heard the news? Our next meetup is on September 10th with Ryan Berry of @Microsoft! RSVP now for free -&gt; http://t.co/QOp26oct4f\n",
      "Positive Tweet: We're excited to learn about #cloud #analytics from @Microsoft tomorrow! Join us https://t.co/p0bMREBBHC #tech #rva http://t.co/1XHmPdSvzq\n",
      "Positive Tweet: Pet adoptions are $10 this Friday @BFAS_LA thanks to @Microsoft @Windows. #Upgradeyourworld with a new best friend! http://t.co/0aI2QiBbCR\n",
      "Positive Tweet: http://t.co/luX5VvBrmJ   Register 4 the NACR Skype for Business event with @Microsoft for Sept 16th Chevy Chase, MD #skype4B #contactcenter\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 Negative Tweets (encoded as 0's)\n",
    "for i,text in enumerate(tweets[tweets['Sentiment'] == 0]['SentimentText']):\n",
    "    print('Negative Tweet:', text)\n",
    "    if i >= 50:\n",
    "        break\n",
    "        \n",
    "# Print first 5 Positive Tweets (encoded as 4's)\n",
    "for i,text in enumerate(tweets[tweets['Sentiment'] == 4]['SentimentText']):\n",
    "    print('Positive Tweet:', text)\n",
    "    if i >= 14:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good idea on using the tokenizer.  we can use this as a function with df.apply to speed this up! Check out the stack overflow solution below for some inspiration.  Some exploratory code is below\n",
    "\n",
    "https://stackoverflow.com/questions/33098040/how-to-use-word-tokenize-in-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'dear'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-47dd6d91c899>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmax_review_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# check `trunc` has expected shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mtrunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \"\"\"\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'dear'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "t = TweetTokenizer()\n",
    "\n",
    "\n",
    "def create_tokens(tweet):\n",
    "    tweet = str(tweet.lower())\n",
    "    tokens = t.tokenize(tweet)\n",
    "    tokens = list(filter(lambda x: not x.startswith('@'), tokens)) ##\n",
    "    tokens = list(filter(lambda x: not x.startswith('#'), tokens)) ##\n",
    "    tokens = list(filter(lambda x: not x.startswith('http'), tokens)) ##\n",
    "    return tokens\n",
    "\n",
    "\n",
    "#Clean SemEval Dataset\n",
    "tweets['SentimentTextTokenized'] = tweets['SentimentText'].apply(create_tokens)\n",
    "tweets.head()\n",
    "\n",
    "#Clean Sentiment140 Dataset\n",
    "tweets2['SentimentTextTokenized'] = tweets2['SentimentText'].apply(create_tokens)\n",
    "tweets2.head()\n",
    "\n",
    "X = tweets.SentimentTextTokenized\n",
    "Y = tweets.Sentiment\n",
    "\n",
    "X2 = tweets2.SentimentTextTokenized\n",
    "Y2 = tweets2.Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "num_bins = 5\n",
    "n, bins, patches = plt.hist(tweets.Sentiment, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Looks like there are only two categories in this Training Dataset. Negative = 0, Positive = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & (future) Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.20,random_state=100)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,Y2,test_size=0.20,random_state=100)\n",
    "print(X_train[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert each word into a vector representation. Couldn't get Keras working with straight indexes for each word so I followed the steps laid out here:\n",
    "# https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html\n",
    "\n",
    "vec_dim = 100\n",
    "\n",
    "tweet_w2v = Word2Vec(size=vec_dim, min_count=5) #vector size and minimum threshold to include for rare words\n",
    "tweet_w2v.build_vocab(x for x in X_train2)\n",
    "tweet_w2v.train((x for x in X_train2), total_examples=tweet_w2v.corpus_count, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_w2v.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Vectors (using SemEval Dataset now)\n",
    "https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, vec_dim) for z in map(lambda x: x, X_train)])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(train_vecs_w2v)\n",
    "# train_vecs_w2v = scaler.transform(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, vec_dim) for z in map(lambda x: x, X_test)])\n",
    "# test_vecs_w2v = scaler.transform(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This section only needed if doing more than binary classification (multi-class)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Original Y:\", y_train[:10])\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test= encoder.transform(y_test)\n",
    "print(\"Encoded Y:\", y_train[:10])\n",
    "\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_test = to_categorical(y_test, 3)\n",
    "print(\"One Hot Y:\", y_train[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model (really simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pad the sequence to the same length (this hurt accuracy?)\n",
    "# train_vecs_w2v = sequence.pad_sequences(train_vecs_w2v, maxlen=vec_dim)\n",
    "# test_vecs_w2v = sequence.pad_sequences(test_vecs_w2v, maxlen=vec_dim)\n",
    "\n",
    "# Build Keras Model\n",
    "\n",
    "# # assemble the embedding_weights in one numpy array\n",
    "# # vocab_dim = 300 # dimensionality of your word vectors\n",
    "# n_symbols = len(index_dict) + 1 # adding 1 to account for 0th index (for masking)\n",
    "# embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "# for word,index in index_dict.items():\n",
    "#     embedding_weights[index, :] = word_vectors[word]\n",
    "\n",
    "# # define inputs here\n",
    "# embedding_layer = Embedding(output_dim=vocab_dim, input_dim=n_symbols, trainable=True)\n",
    "# embedding_layer.build((None,)) # if you don't do this, the next step won't work\n",
    "# embedding_layer.set_weights([embedding_weights])\n",
    "\n",
    "# embedded = embedding_layer(input_layer)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=vec_dim))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax')) # softmax if multi-class\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy', # categorical_crossentropy if multi-class\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.add(Embedding(top_words, vec_dim, input_length=max_review_length))\n",
    "# model.add(Conv1D(64, 3, padding='same'))\n",
    "# model.add(Conv1D(32, 3, border_mode='same'))\n",
    "# model.add(Conv1D(16, 3, border_mode='same'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(180,activation='sigmoid'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1,activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=32, verbose=2)\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without Tweet Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_vecs_w2v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tweet_w2v.wv.vocab), vec_dim))\n",
    "for i in range(len(tweet_w2v.wv.vocab)):\n",
    "    embedding_vector = tweet_w2v.wv[tweet_w2v.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "# model.add(Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n",
    "#                       weights=[embedding_matrix]))\n",
    "\n",
    "# Using embedding from Keras\n",
    "# embedding_vecor_length = 300\n",
    "# max_review_length = 30\n",
    "# train_vecs_w2v = sequence.pad_sequences(train_vecs_w2v, maxlen=max_review_length)\n",
    "# test_vecs_w2v = sequence.pad_sequences(test_vecs_w2v, maxlen=max_review_length)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "\n",
    "# model.add(Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], input_length=max_review_length,\n",
    "#                       weights=[embedding_matrix]))\n",
    "# model.add(Dense(32, activation='relu', input_dim=vec_dim))\n",
    "\n",
    "# Convolutional model (3x conv, flatten, 2x dense)\n",
    "model.add(Conv1D(64, 3, padding='same'))\n",
    "model.add(Conv1D(32, 3, padding='same'))\n",
    "model.add(Conv1D(16, 3, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(180,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy', # categorical_crossentropy if multi-class\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, nb_epoch=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using keras to load the dataset with the top_words\n",
    "top_words = 10000\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "\n",
    "# Pad the sequence to the same length\n",
    "max_review_length = 40\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# Using embedding from Keras\n",
    "embedding_vector_length = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "\n",
    "# Convolutional model (3x conv, flatten, 2x dense)\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Log to tensorboard\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
    "\n",
    "# Evaluation on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to Predict Positive/Neutral/Negative\n",
    "\n",
    "def prediction(text):\n",
    "    sentiment = [\"Negative\",'Neutral', \"Positive\"]\n",
    "    tokens = create_tokens(text)\n",
    "    vecs = buildWordVector(tokens, vec_dim)\n",
    "    vecs = scaler.transform(vecs)\n",
    "    predic = model.predict(vecs)\n",
    "    result = sentiment[predic.argmax(axis=1)[0]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "#this is a read-only instance\n",
    "reddit = praw.Reddit(user_agent='first_scrape (by /u/dswald)',\n",
    "                     client_id='TyAK1zSuAvQjmA', \n",
    "                     client_secret=\"uxHGsL0zNODbowN6umVnBWpqLAQ\")\n",
    "\n",
    "subreddit = reddit.subreddit('tensorflow')\n",
    "hot_python = subreddit.hot(limit = 3) #need to view >2 to get past promoted posts\n",
    "\n",
    "for submission in hot_python:\n",
    "    if not submission.stickied: #top 2 are promoted posts, labeled as 'stickied'\n",
    "        print('Title: {}, ups: {}, downs: {}, Have we visited: {}'.format(submission.title,\n",
    "                                                                          submission.ups,\n",
    "                                                                          submission.downs,\n",
    "                                                                          submission.visited))\n",
    "        comments = submission.comments.list() #unstructured\n",
    "        for comment in comments:\n",
    "            print (20*'-')\n",
    "            print ('Parent ID:', comment.parent())\n",
    "            print ('Comment ID:', comment.id)\n",
    "            print (comment.body)\n",
    "            print(\"#\"*10,'PREDICTED SENTIMENT:', prediction(comment.body),\"#\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that everythng is positive or neutral.  Is this because of a unigram model or too specific training data or what?  This is food for thought.  Committing now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
