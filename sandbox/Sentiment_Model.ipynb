{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Sentiment Predicting Model on a Social Media Corpus\n",
    "\n",
    "### Using the SemEval 2017 Task 4A: Positive/Negative/Neutral Classifier Corpus\n",
    "\n",
    "This model does not incorporate vector word embeddings or any smoothing. \n",
    "\n",
    "Helpful Resources:\n",
    "https://medium.com/@thoszymkowiak/how-to-implement-sentiment-analysis-using-word-embedding-and-convolutional-neural-networks-on-keras-163197aef623\n",
    "\n",
    "SemEval Tweet Download: \n",
    "https://github.com/seirasto/twitter_download\n",
    "\n",
    "Good Post I found that most of this code is built off of\n",
    "https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html\n",
    "\n",
    "Potential other corpus to use (1.6million tweets as positive/negative)\n",
    "https://drive.google.com/uc?id=0B04GJPshIjmPRnZManQwWEdTZjg&export=download\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard python helper libraries.\n",
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import scipy.optimize\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Helper libraries (from w266 Materials).\n",
    "# import segment\n",
    "from shared_lib import utils\n",
    "from shared_lib import vocabulary\n",
    "\n",
    "# Machine Learning Packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Word2Vec Model\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Conv1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628949369883000832</td>\n",
       "      <td>negative</td>\n",
       "      <td>dear @Microsoft the newOoffice for Mac is grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>628976607420645377</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Microsoft how about you make a system that do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>629023169169518592</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>629179223232479232</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>629186282179153920</td>\n",
       "      <td>neutral</td>\n",
       "      <td>If I make a game as a #windows10 Universal App...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>629226490152914944</td>\n",
       "      <td>positive</td>\n",
       "      <td>Microsoft, I may not prefer your gaming branch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>629345637155360768</td>\n",
       "      <td>negative</td>\n",
       "      <td>@MikeWolf1980 @Microsoft I will be downgrading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>629394528336637953</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Microsoft 2nd computer with same error!!! #Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>629650766580609026</td>\n",
       "      <td>positive</td>\n",
       "      <td>Just ordered my 1st ever tablet; @Microsoft Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>629797991826722816</td>\n",
       "      <td>negative</td>\n",
       "      <td>After attempting a reinstall, it still bricks,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>630159517058142208</td>\n",
       "      <td>positive</td>\n",
       "      <td>Sunday morning, quiet day so time to welcome i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>630542330827771904</td>\n",
       "      <td>negative</td>\n",
       "      <td>Did @Microsoft break Windows 10? Was working f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>630636736746422272</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>630807124872970240</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@spyderharrison @Microsoft the reason I ask is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>630818265799921664</td>\n",
       "      <td>positive</td>\n",
       "      <td>Innovation for jobs is just around the corner ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>630909171437801472</td>\n",
       "      <td>neutral</td>\n",
       "      <td>OK this is my pure speculation.  @Microsoft ow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>630982270409572352</td>\n",
       "      <td>neutral</td>\n",
       "      <td>We are still taking registrations for our Educ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>631104156187627520</td>\n",
       "      <td>negative</td>\n",
       "      <td>For the 1st time @Skype has a \"High Startup im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>631223085476261890</td>\n",
       "      <td>negative</td>\n",
       "      <td>#teens @BillGates 1st company failed miserably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>631368262979297281</td>\n",
       "      <td>positive</td>\n",
       "      <td>#Vote for @AIESEC to become the 10th Global no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>631521079245307904</td>\n",
       "      <td>positive</td>\n",
       "      <td>Top 5 most searched for Back-to-School topics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>631543121407442946</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Microsoft support for 365 has been terrible. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>631696872323850240</td>\n",
       "      <td>positive</td>\n",
       "      <td>@trucker_squigz @Microsoft @MISpeedway @nation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>631792365590695936</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>631842974268305408</td>\n",
       "      <td>positive</td>\n",
       "      <td>@ScottArbeit @GabeAul @Microsoft isntall the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>631843393971204097</td>\n",
       "      <td>positive</td>\n",
       "      <td>@taehongmin1 We have an IOT workshop by @Micro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>631936716522278912</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@kenttaylor333 @YourAnonNews @Microsoft the op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>632374683334258688</td>\n",
       "      <td>negative</td>\n",
       "      <td>Hey @Microsoft, I reserved my copy of @Windows...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>632536348419690496</td>\n",
       "      <td>negative</td>\n",
       "      <td>@eyesonfoxorg @Microsoft I'm still using Vista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>632805868334153728</td>\n",
       "      <td>negative</td>\n",
       "      <td>@Microsoft Is it normal that it takes hours to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5861</th>\n",
       "      <td>634021249283420161</td>\n",
       "      <td>positive</td>\n",
       "      <td>We'll run it back. But it's STH day here in Fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5862</th>\n",
       "      <td>634061328550883328</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5863</th>\n",
       "      <td>634184470338383873</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>634353168596340736</td>\n",
       "      <td>neutral</td>\n",
       "      <td>It's the Atlanta Falcons (1-0) against the New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5865</th>\n",
       "      <td>634355631940612097</td>\n",
       "      <td>positive</td>\n",
       "      <td>Who's ready for some #GiantsFootball? The G-Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>634372935541592065</td>\n",
       "      <td>positive</td>\n",
       "      <td>I'll be live streaming the sad Giants fans via...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5867</th>\n",
       "      <td>634412669727047681</td>\n",
       "      <td>positive</td>\n",
       "      <td>I can't wait to see the Giants this Saturday a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5868</th>\n",
       "      <td>634762357563129857</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5869</th>\n",
       "      <td>634800750833594368</td>\n",
       "      <td>positive</td>\n",
       "      <td>I have one possibly two extra tickets to see Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5870</th>\n",
       "      <td>634902054562086912</td>\n",
       "      <td>neutral</td>\n",
       "      <td>im going to metlife tomorrow for a preseason g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5871</th>\n",
       "      <td>635129647747989504</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@RawbCas3 Alright, let me know. Want to see AC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>635460536445063168</td>\n",
       "      <td>positive</td>\n",
       "      <td>Going to the Giants-Panthers game December 20 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>635599514556956672</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>635658990123360258</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>636324904258904065</td>\n",
       "      <td>positive</td>\n",
       "      <td>Looks like I am going to see my phins at Gille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>636714456546975744</td>\n",
       "      <td>positive</td>\n",
       "      <td>Oney Thursday and Friday, MetLife for Giants-J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5877</th>\n",
       "      <td>637340624031604736</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>637342059519680513</td>\n",
       "      <td>negative</td>\n",
       "      <td>Still bitter that they didn't tweet about MetL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>637691185100947456</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>637874723288936448</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>638382158420307968</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>638533993344864256</td>\n",
       "      <td>positive</td>\n",
       "      <td>Schreier Financial Services in Orange City wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>639166904813223937</td>\n",
       "      <td>positive</td>\n",
       "      <td>Heading up to MetLife tomorrow for the Jets game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>639295526995890177</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5885</th>\n",
       "      <td>639804828739346432</td>\n",
       "      <td>positive</td>\n",
       "      <td>It's the first Football Friday of the year. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5886</th>\n",
       "      <td>639855845958885376</td>\n",
       "      <td>positive</td>\n",
       "      <td>@Racalto_SK ok good to know. Punting at MetLif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5887</th>\n",
       "      <td>639979760735662080</td>\n",
       "      <td>neutral</td>\n",
       "      <td>everyone who sat around me at metlife was so a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5888</th>\n",
       "      <td>640196838260363269</td>\n",
       "      <td>neutral</td>\n",
       "      <td>what giants or niners fans would wanna go to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5889</th>\n",
       "      <td>640975710354567168</td>\n",
       "      <td>positive</td>\n",
       "      <td>Anybody want a ticket for tomorrow Colombia vs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5890</th>\n",
       "      <td>641034340068143104</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Mendez told me he'd drive me to MetLife on Sun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5891 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0         1  \\\n",
       "0     628949369883000832  negative   \n",
       "1     628976607420645377  negative   \n",
       "2     629023169169518592  negative   \n",
       "3     629179223232479232  negative   \n",
       "4     629186282179153920   neutral   \n",
       "5     629226490152914944  positive   \n",
       "6     629345637155360768  negative   \n",
       "7     629394528336637953  negative   \n",
       "8     629650766580609026  positive   \n",
       "9     629797991826722816  negative   \n",
       "10    630159517058142208  positive   \n",
       "11    630542330827771904  negative   \n",
       "12    630636736746422272  negative   \n",
       "13    630807124872970240   neutral   \n",
       "14    630818265799921664  positive   \n",
       "15    630909171437801472   neutral   \n",
       "16    630982270409572352   neutral   \n",
       "17    631104156187627520  negative   \n",
       "18    631223085476261890  negative   \n",
       "19    631368262979297281  positive   \n",
       "20    631521079245307904  positive   \n",
       "21    631543121407442946  negative   \n",
       "22    631696872323850240  positive   \n",
       "23    631792365590695936  negative   \n",
       "24    631842974268305408  positive   \n",
       "25    631843393971204097  positive   \n",
       "26    631936716522278912   neutral   \n",
       "27    632374683334258688  negative   \n",
       "28    632536348419690496  negative   \n",
       "29    632805868334153728  negative   \n",
       "...                  ...       ...   \n",
       "5861  634021249283420161  positive   \n",
       "5862  634061328550883328  positive   \n",
       "5863  634184470338383873  positive   \n",
       "5864  634353168596340736   neutral   \n",
       "5865  634355631940612097  positive   \n",
       "5866  634372935541592065  positive   \n",
       "5867  634412669727047681  positive   \n",
       "5868  634762357563129857   neutral   \n",
       "5869  634800750833594368  positive   \n",
       "5870  634902054562086912   neutral   \n",
       "5871  635129647747989504   neutral   \n",
       "5872  635460536445063168  positive   \n",
       "5873  635599514556956672  positive   \n",
       "5874  635658990123360258  positive   \n",
       "5875  636324904258904065  positive   \n",
       "5876  636714456546975744  positive   \n",
       "5877  637340624031604736  negative   \n",
       "5878  637342059519680513  negative   \n",
       "5879  637691185100947456  positive   \n",
       "5880  637874723288936448  positive   \n",
       "5881  638382158420307968   neutral   \n",
       "5882  638533993344864256  positive   \n",
       "5883  639166904813223937  positive   \n",
       "5884  639295526995890177  positive   \n",
       "5885  639804828739346432  positive   \n",
       "5886  639855845958885376  positive   \n",
       "5887  639979760735662080   neutral   \n",
       "5888  640196838260363269   neutral   \n",
       "5889  640975710354567168  positive   \n",
       "5890  641034340068143104   neutral   \n",
       "\n",
       "                                                      2  \n",
       "0     dear @Microsoft the newOoffice for Mac is grea...  \n",
       "1     @Microsoft how about you make a system that do...  \n",
       "2                                         Not Available  \n",
       "3                                         Not Available  \n",
       "4     If I make a game as a #windows10 Universal App...  \n",
       "5     Microsoft, I may not prefer your gaming branch...  \n",
       "6     @MikeWolf1980 @Microsoft I will be downgrading...  \n",
       "7     @Microsoft 2nd computer with same error!!! #Wi...  \n",
       "8     Just ordered my 1st ever tablet; @Microsoft Su...  \n",
       "9     After attempting a reinstall, it still bricks,...  \n",
       "10    Sunday morning, quiet day so time to welcome i...  \n",
       "11    Did @Microsoft break Windows 10? Was working f...  \n",
       "12                                        Not Available  \n",
       "13    @spyderharrison @Microsoft the reason I ask is...  \n",
       "14    Innovation for jobs is just around the corner ...  \n",
       "15    OK this is my pure speculation.  @Microsoft ow...  \n",
       "16    We are still taking registrations for our Educ...  \n",
       "17    For the 1st time @Skype has a \"High Startup im...  \n",
       "18    #teens @BillGates 1st company failed miserably...  \n",
       "19    #Vote for @AIESEC to become the 10th Global no...  \n",
       "20    Top 5 most searched for Back-to-School topics ...  \n",
       "21    @Microsoft support for 365 has been terrible. ...  \n",
       "22    @trucker_squigz @Microsoft @MISpeedway @nation...  \n",
       "23                                        Not Available  \n",
       "24    @ScottArbeit @GabeAul @Microsoft isntall the n...  \n",
       "25    @taehongmin1 We have an IOT workshop by @Micro...  \n",
       "26    @kenttaylor333 @YourAnonNews @Microsoft the op...  \n",
       "27    Hey @Microsoft, I reserved my copy of @Windows...  \n",
       "28    @eyesonfoxorg @Microsoft I'm still using Vista...  \n",
       "29    @Microsoft Is it normal that it takes hours to...  \n",
       "...                                                 ...  \n",
       "5861  We'll run it back. But it's STH day here in Fl...  \n",
       "5862                                      Not Available  \n",
       "5863                                      Not Available  \n",
       "5864  It's the Atlanta Falcons (1-0) against the New...  \n",
       "5865  Who's ready for some #GiantsFootball? The G-Me...  \n",
       "5866  I'll be live streaming the sad Giants fans via...  \n",
       "5867  I can't wait to see the Giants this Saturday a...  \n",
       "5868                                      Not Available  \n",
       "5869  I have one possibly two extra tickets to see Z...  \n",
       "5870  im going to metlife tomorrow for a preseason g...  \n",
       "5871  @RawbCas3 Alright, let me know. Want to see AC...  \n",
       "5872  Going to the Giants-Panthers game December 20 ...  \n",
       "5873                                      Not Available  \n",
       "5874                                      Not Available  \n",
       "5875  Looks like I am going to see my phins at Gille...  \n",
       "5876  Oney Thursday and Friday, MetLife for Giants-J...  \n",
       "5877                                      Not Available  \n",
       "5878  Still bitter that they didn't tweet about MetL...  \n",
       "5879                                      Not Available  \n",
       "5880                                      Not Available  \n",
       "5881                                      Not Available  \n",
       "5882  Schreier Financial Services in Orange City wil...  \n",
       "5883   Heading up to MetLife tomorrow for the Jets game  \n",
       "5884                                      Not Available  \n",
       "5885  It's the first Football Friday of the year. Th...  \n",
       "5886  @Racalto_SK ok good to know. Punting at MetLif...  \n",
       "5887  everyone who sat around me at metlife was so a...  \n",
       "5888  what giants or niners fans would wanna go to t...  \n",
       "5889  Anybody want a ticket for tomorrow Colombia vs...  \n",
       "5890  Mendez told me he'd drive me to MetLife on Sun...  \n",
       "\n",
       "[5891 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull in Tweet Data (Must be downloaded using https://github.com/seirasto/twitter_download)\n",
    "tweets = pd.read_table(\"Data/twitter_download-master/2016train.txt_semeval_tweets.txt\", header=None)\n",
    "tweets\n",
    "\n",
    "# SemEval Dataset is actually relatively small (6000 tweets in 2016). \n",
    "# We can group all of the Train/Test/Dev data from 2013 through 2016 to get more.\n",
    "# Additionally, we could consider using this data which has 1.6 million rows but it is only a binary positive/negative class \n",
    "# https://drive.google.com/uc?id=0B04GJPshIjmPRnZManQwWEdTZjg&export=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregate X and Y\n",
    "X = tweets[2]\n",
    "Y = tweets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexing.py:601: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each Tweet (really slow, need to optimize for larger corpora?)\n",
    "for i, tweet in enumerate(X):\n",
    "    X[i,] = tweet.split()\n",
    "\n",
    "\n",
    "# print(X)\n",
    "\n",
    "# Alternatively, use this?\n",
    "# from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "# tokenizer = TweetTokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & (future) Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4395    [#Apology, to, Jeb, Bush, for, John, Dempsey, ...\n",
      "4068    [Top, 5, Gambling, Apps, for, the, iPad, http:...\n",
      "3710                                     [Not, Available]\n",
      "4516    [@Milbank, doesn't, think, Vice, President, Jo...\n",
      "4243    [1st, day, back, at, work, after, a, terrible,...\n",
      "4288                                     [Not, Available]\n",
      "4916                                     [Not, Available]\n",
      "1360    [1), may, be, wrong,, but, if, I, read, it, ri...\n",
      "1012    [@GailSimone, Donald, Trump, may, think, he's,...\n",
      "1538    [#nowplaying, Bob, Marley, -, Sun, Is, Shining...\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.20,random_state=100)\n",
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49155"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each word into a vector representation. Couldn't get Keras working with straight indexes for each word so I followed the steps laid out here:\n",
    "# https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html\n",
    "\n",
    "vec_dim = 10\n",
    "\n",
    "tweet_w2v = Word2Vec(size=vec_dim, min_count=2) #vector size and minimum threshold to include for rare words\n",
    "tweet_w2v.build_vocab(x for x in X_train)\n",
    "tweet_w2v.train((x for x in X_train), total_examples=tweet_w2v.corpus_count, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('livestream', 0.8935967683792114),\n",
       " ('Friday:', 0.8406975269317627),\n",
       " ('recent', 0.795516312122345),\n",
       " (\"it'd\", 0.7905986905097961),\n",
       " ('fucked', 0.7877628803253174),\n",
       " ('books', 0.778687059879303),\n",
       " ('point', 0.7704617977142334),\n",
       " ('&amp;...', 0.770136833190918),\n",
       " ('happiest', 0.7694127559661865),\n",
       " ('Edgar', 0.7674282789230347)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Vectors \n",
    "https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = numpy.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_vecs_w2v = numpy.concatenate([buildWordVector(z, vec_dim) for z in map(lambda x: x, X_train)])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_vecs_w2v)\n",
    "train_vecs_w2v = scaler.transform(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = numpy.concatenate([buildWordVector(z, vec_dim) for z in map(lambda x: x, X_test)])\n",
    "test_vecs_w2v = scaler.transform(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Y: 4395     neutral\n",
      "4068     neutral\n",
      "3710    positive\n",
      "4516     neutral\n",
      "4243    negative\n",
      "4288     neutral\n",
      "4916     neutral\n",
      "1360     neutral\n",
      "1012     neutral\n",
      "1538    positive\n",
      "Name: 1, dtype: object\n",
      "Encoded Y: [1 1 2 1 0 1 1 1 1 2]\n",
      "One Hot Y: [[ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Original Y:\", y_train[:10])\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test= encoder.transform(y_test)\n",
    "print(\"Encoded Y:\", y_train[:10])\n",
    "\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_test = to_categorical(y_test, 3)\n",
    "print(\"One Hot Y:\", y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model (really simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      " - 0s - loss: 1.0243 - acc: 0.4945\n",
      "Epoch 2/9\n",
      " - 0s - loss: 0.9949 - acc: 0.5119\n",
      "Epoch 3/9\n",
      " - 0s - loss: 0.9882 - acc: 0.5157\n",
      "Epoch 4/9\n",
      " - 0s - loss: 0.9840 - acc: 0.5142\n",
      "Epoch 5/9\n",
      " - 0s - loss: 0.9819 - acc: 0.5151\n",
      "Epoch 6/9\n",
      " - 0s - loss: 0.9799 - acc: 0.5153\n",
      "Epoch 7/9\n",
      " - 0s - loss: 0.9780 - acc: 0.5174\n",
      "Epoch 8/9\n",
      " - 0s - loss: 0.9778 - acc: 0.5174\n",
      "Epoch 9/9\n",
      " - 0s - loss: 0.9760 - acc: 0.5191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x197f0f8a550>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the sequence to the same length\n",
    "# train_vecs_w2v = sequence.pad_sequences(train_vecs_w2v, maxlen=vec_dim)\n",
    "# test_vecs_w2v = sequence.pad_sequences(test_vecs_w2v, maxlen=vec_dim)\n",
    "\n",
    "# Build Keras Model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=vec_dim))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.64%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM for sequence classification\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM, Conv1D, Flatten, Dropout\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# from keras.preprocessing import sequence\n",
    "# from keras.callbacks import TensorBoard\n",
    "\n",
    "# # # Using keras to load the dataset with the top_words\n",
    "# # top_words = 10000\n",
    "\n",
    "\n",
    "# # # Pad the sequence to the same length\n",
    "# max_review_length = vec_dim\n",
    "# X_train = sequence.pad_sequences(train_vecs_w2v, maxlen=max_review_length)\n",
    "# X_test = sequence.pad_sequences(test_vecs_w2v, maxlen=max_review_length)\n",
    "\n",
    "# # Using embedding from Keras\n",
    "# # embedding_vecor_length = 300\n",
    "# model = Sequential()\n",
    "# # model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "\n",
    "# # Convolutional model (3x conv, flatten, 2x dense)\n",
    "# model.add(Conv1D(64, 3, padding='same', input_shape=(None, vec_dim)))\n",
    "# model.add(Conv1D(32, 3, padding='same'))\n",
    "# model.add(Conv1D(16, 3, padding='same'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(180,activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1,activation='softmax'))\n",
    "\n",
    "# # Log to tensorboard\n",
    "# tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(train_vecs_w2v, y_train, nb_epoch=3, callbacks=[tensorBoardCallback], batch_size=64)\n",
    "\n",
    "# # Evaluation on the test set\n",
    "# scores = model.evaluate(test_vecs_w2v, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to Predict Positive/Neutral/Negative\n",
    "\n",
    "def prediction(text):\n",
    "    sentiment = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "    text = text.split() # Tokenize\n",
    "    text = buildWordVector(text, vec_dim)\n",
    "    text = scaler.transform(text)\n",
    "    predic = model.predict(text, batch_size=32)\n",
    "    result = sentiment[predic.argmax(axis=1)[0]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Happy Whalesplosion Day!, ups: 349, downs: 0, Have we visited: False\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dppspoz\n",
      ">THE BLAST BLASTED BLUBBER BEYOND ALL BELIEVABLE BOUNDS!\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dpprm5u\n",
      "What I would love, more than anything, would be to get a hold of the insurance claim made on [this car.](http://www.salem-news.com/nphotos/smashed-car.jpg)\n",
      "\n",
      "That damage was caused by flying whale blubber.  That claim has to be framed in an insurance office somewhere as the weirdest claim they've ever seen.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dppr9fs\n",
      "I remember first learning about this and being so stunned. After watching the video I was like sad and disgusted but also... so fucking amused- what a ridiculous plan. real life slap-stick\n",
      "\n",
      "I'm all about respecting the corpses of all creatures and stuff, but like, the effort was sincere and the effect was hilarious- I bet that whale is laughing it's ass off in sea-heaven or wherever tf it is now\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dppwcbs\n",
      "This will never cease to be hilarious\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dpps7kz\n",
      "[Here's an excellent episode of The Dollop about whalesplosion](http://thedollop.libsyn.com/227-whalesplosion)\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dppy7xc\n",
      "The cutscene to the highway engineer after the blast flipping through his notepad, looking like he's going over calculations and wondering where it went so, so wrong..\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dppyc0d\n",
      "Did anyone else hear that loud bang outside my beach house 47 years ago?\n",
      "########## PREDICTED SENTIMENT: Neutral ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dppvq0v\n",
      "My hippy mom watched it happen. Every year she describes the smell in great detail. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dpq0zla\n",
      "This is one of my favorite videos ever. The sound when it all starts coming down gets me every time. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dppvt9u\n",
      "Lol. Awesome. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dpq39qk\n",
      "I am having a shitty afternoon and the title of this post made me giggle. Thank you\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dpq14fy\n",
      "I often wonder what other prospects for carcass disposal on that scale could have been used. Flamethrowers? Chainsaws? \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dpq254a\n",
      "I wonder what the best protocol is nowdays for something like this.  I saw a special about a sperm whale in taiwan in the early 2000s.  They got it loaded up on a truck to take it to autopsy.  In the middle of a downtown area, the whale ruptured and rotten guts flooded the streets.  I'd rather have Whalesplosion.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dpqa93n\n",
      "That was so cool\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dpqedxj\n",
      "I had a teacher in college who made a film about this, not many people have probably seen it.  \n",
      "\n",
      "[Here it is.](https://media.uoregon.edu/channel/archives/3218)\n",
      "\n",
      "\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dpqhk5t\n",
      "Oh god, thank you for posting this. I forgot about this for a whole year. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7cgsh0\n",
      "Comment ID: dppsrdw\n",
      "Would be considered a terrorist act in today's nurtured society. But God, wish I was alive at the time. Probably would of ran up and kicked it a few times... \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppspoz\n",
      "Comment ID: dpq2ipz\n",
      "I need this on a bumper sticker. :D\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppspoz\n",
      "Comment ID: dpq3vp9\n",
      "Also: \"governmental blubber flub-up.\" \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppspoz\n",
      "Comment ID: dpq2lix\n",
      "How about:\n",
      "\n",
      "> Blast Blows Blubber Beyond Believable Bounds\n",
      "########## PREDICTED SENTIMENT: Neutral ##########\n",
      "--------------------\n",
      "Parent ID: dpprm5u\n",
      "Comment ID: dppxdkp\n",
      "What's the best ICD-10 diagnosis code for getting hit by flying whale blubber? \n",
      "\n",
      "I'm going have to go with W20.8XXA: Other cause of strike by thrown, projected or falling object, initial encounter\n",
      "\n",
      "Honorable mention to W56.32XA: Contact with nonvenomous marine animal, struck by other marine mammal, initial encounter\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpprm5u\n",
      "Comment ID: dpq3zuq\n",
      "Bought under a \"Get a Whale of a Deal\" promotion from a nearby town, too, according to Offbeat Oregon (http://offbeatoregon.com/1608bT.exploding-whale.404.html) and Wiki. Too hilarious! \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpprm5u\n",
      "Comment ID: dppy3dg\n",
      "[I'm just gonna leave this here.](https://m.youtube.com/watch?v=YCsfHVM5x_I)\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpps7kz\n",
      "Comment ID: dpq5zng\n",
      "Came here to post this, im glad I don't have to do the work lol\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppvq0v\n",
      "Comment ID: dppx1u3\n",
      "Go on.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpq14fy\n",
      "Comment ID: dpq2oap\n",
      "Burial? Let the little beach creatures eat it. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpq254a\n",
      "Comment ID: dpq4ybk\n",
      "A blue whale carcass washed up in 2015. They were segmenting it and planning to then set the bones in a field for cleaning/bleaching for eventual display in the Hatfield Science Center in Newport as I recall. I don’t think they’d do this for any whale carcass but a blue whale is something special. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppsrdw\n",
      "Comment ID: dppug3m\n",
      "The proper move when finding any dead body/animal is to poke it with a stick. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppsrdw\n",
      "Comment ID: dppw7tz\n",
      "Ah dude nice you seem awesome\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppsrdw\n",
      "Comment ID: dpq0ooe\n",
      "The Marine Mammal Protection Act says you can't even harass dead ones.  \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpq2ipz\n",
      "Comment ID: dpqjeiq\n",
      "[Actual video of the event](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppxdkp\n",
      "Comment ID: dppxmbg\n",
      "Maybe W40.0XXD - Explosion of blasting material, subsequent encounter?\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppy3dg\n",
      "Comment ID: dpqjbcr\n",
      "[THIS IS THE ACTUAL VIDEO](https://youtu.be/xBgThvB_IDQ?t=121)\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppx1u3\n",
      "Comment ID: dppxo8f\n",
      "The permeating stench of sea salt and rot. Like you smell by a cannery but magnified by a thousand. Like breathing small particles of the exploded whale, which in fact they were doing as they were fairly close to the carcass.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpq2oap\n",
      "Comment ID: dpq3k46\n",
      "Wikipedia says standard practice of whale disposal nowadays is to take the corpse out a good distance to sea first and then blow it up. I guess there's a strong risk of the whale exploding on its own from gas buildup as it decomposes, which impacts people trying to cut up the body. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpq4ybk\n",
      "Comment ID: dpqese2\n",
      "Somehow I forgot about this.  Do you know what they ended up doing?  We took the kids to Hatfield this summer and I don't recall seeing anything more than a partial jaw or skull\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppug3m\n",
      "Comment ID: dppx24m\n",
      "[Maybe not the best move when it comes to dead whales. [NSFWhale/Life]](https://i.imgur.com/qlxYwBZ.gifv)\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppug3m\n",
      "Comment ID: dppwkkd\n",
      "Too large to move with a stick. You'd need a whole doug fir.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppug3m\n",
      "Comment ID: dpq0duy\n",
      "[Absolutely](https://i.imgur.com/vDONJI2.jpg)\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppug3m\n",
      "Comment ID: dppv0su\n",
      "perhaps, but kicking in the face gives you that high...like picking a scab. ooooooh\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpq0ooe\n",
      "Comment ID: dpq0uhi\n",
      "capitol punishment, I believe. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppxo8f\n",
      "Comment ID: dppxv5r\n",
      "Thank you. Your mom has a way with words.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppxo8f\n",
      "Comment ID: dpqalie\n",
      "ur mom had that whale in her, cool.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpqese2\n",
      "Comment ID: dpqfb4j\n",
      "Unsure. It was a lot of whale\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppx24m\n",
      "Comment ID: dppzdfu\n",
      "There’s a show called [Inside Nature’s Giants](https://en.wikipedia.org/wiki/Inside_Nature%27s_Giants) and [the sperm whale episode is on YouTube.](https://youtu.be/aXccTHXPYfM) I highly recommend checking it out.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppwkkd\n",
      "Comment ID: dppx09r\n",
      "Who said anything about moving it? I️ just want to poke it with a stick. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpq0duy\n",
      "Comment ID: dpqc4rp\n",
      "if that were whale vomit, it could be worth a nice chunk of change. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppxv5r\n",
      "Comment ID: dppy0k5\n",
      "Not my real mom, just the mom of my hippy family. But I'll tell her you appreciate her descriptive ways.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpqalie\n",
      "Comment ID: dpqbat1\n",
      "Not *really* my mom. But I can respect a good \"your Mom\" joke. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppx09r\n",
      "Comment ID: dppx1to\n",
      "True, I just assumed that. Poke away!\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dppy0k5\n",
      "Comment ID: dpq0wbm\n",
      "I didn't pick up on that being something different than an adjective to describe your mother, but thanks to both of you.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpqbat1\n",
      "Comment ID: dpqcm8x\n",
      "Oh sorry I wasn't implying a ur mom joke, and I didn't see that whole not ur actual mom thing. I am just typing on mobile and being lazy but for real it's cool she had a blown up whale in her from breathing it in lol \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpq0wbm\n",
      "Comment ID: dpq3vnd\n",
      "I'm a coastie by birth. It's different down there. 😎\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpqcm8x\n",
      "Comment ID: dpqd0dp\n",
      "Lol totally.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpq3vnd\n",
      "Comment ID: dpq5ed0\n",
      "Florence? Florence is beautiful. \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dpq5ed0\n",
      "Comment ID: dpq7dbb\n",
      "I wish. Coos Bay.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "#this is a read-only instance\n",
    "reddit = praw.Reddit(user_agent='first_scrape (by /u/dswald)',\n",
    "                     client_id='TyAK1zSuAvQjmA', \n",
    "                     client_secret=\"uxHGsL0zNODbowN6umVnBWpqLAQ\")\n",
    "\n",
    "subreddit = reddit.subreddit('Portland')\n",
    "hot_python = subreddit.hot(limit = 3) #need to view >2 to get past promoted posts\n",
    "\n",
    "for submission in hot_python:\n",
    "    if not submission.stickied: #top 2 are promoted posts, labeled as 'stickied'\n",
    "        print('Title: {}, ups: {}, downs: {}, Have we visited: {}'.format(submission.title,\n",
    "                                                                          submission.ups,\n",
    "                                                                          submission.downs,\n",
    "                                                                          submission.visited))\n",
    "        comments = submission.comments.list() #unstructured\n",
    "        for comment in comments:\n",
    "            print (20*'-')\n",
    "            print ('Parent ID:', comment.parent())\n",
    "            print ('Comment ID:', comment.id)\n",
    "            print (comment.body)\n",
    "            print(\"#\"*10,'PREDICTED SENTIMENT:', prediction(comment.body),\"#\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
