{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Sentiment Predicting Model on a Social Media Corpus\n",
    "\n",
    "### Using the 1.6 Million Tweets Positive/Negative Sentiment Corpus\n",
    "\n",
    "\n",
    "Helpful Resources:\n",
    "https://medium.com/@thoszymkowiak/how-to-implement-sentiment-analysis-using-word-embedding-and-convolutional-neural-networks-on-keras-163197aef623\n",
    "\n",
    "Good Post I found that most of this code is built off of\n",
    "https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html\n",
    "\n",
    "Corpus (1.6million tweets as positive/negative)\n",
    "https://drive.google.com/uc?id=0B04GJPshIjmPRnZManQwWEdTZjg&export=download\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import scipy.optimize\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Helper libraries (from w266 Materials).\n",
    "# import segment\n",
    "#from shared_lib import utils\n",
    "from shared_lib import vocabulary\n",
    "\n",
    "# Machine Learning Packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Word2Vec Model\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Conv1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (1600000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull in Tweet Data (Must be downloaded using https://github.com/seirasto/twitter_download)\n",
    "# tweets = pd.read_csv(\"Data/milliontweetCorpus/training.1600000.processed.noemoticon.csv\", header=None)\n",
    "# tweets\n",
    "\n",
    "cols = ['Sentiment', 'ItemID', 'DateTime', 'Query', 'SentimentSource', 'SentimentText']\n",
    "# num_tweets = 1000000\n",
    "tweets = pd.read_csv('Data/milliontweetCorpus/training.1600000.processed.noemoticon.csv', \n",
    "                     header=None, names=cols, encoding='ISO-8859-1', error_bad_lines=False) #, nrows=num_tweets)\n",
    "tweets.drop(['ItemID', 'DateTime', 'Query', 'SentimentSource'], axis=1, inplace=True)\n",
    "tweets = tweets[tweets.Sentiment.isnull() == False]\n",
    "tweets['Sentiment'] = tweets['Sentiment'].map(int)\n",
    "tweets = tweets[tweets['SentimentText'].isnull() == False]\n",
    "tweets.reset_index(inplace=True)\n",
    "tweets.drop('index', axis=1, inplace=True)\n",
    "print('dataset loaded with shape', tweets.shape)\n",
    "tweets[tweets['Sentiment'] == 0].head(5)\n",
    "# SemEval Dataset is actually relatively small (6000 tweets in 2016). \n",
    "# We can group all of the Train/Test/Dev data from 2013 through 2016 to get more.\n",
    "# Additionally, we could consider using this data which has 1.6 million rows but it is only a binary positive/negative class \n",
    "# https://drive.google.com/uc?id=0B04GJPshIjmPRnZManQwWEdTZjg&export=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Tweet: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "Negative Tweet: is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "Negative Tweet: @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
      "Negative Tweet: my whole body feels itchy and like its on fire \n",
      "Negative Tweet: @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n",
      "Negative Tweet: @Kwesidei not the whole crew \n",
      "Negative Tweet: Need a hug \n",
      "Negative Tweet: @LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\n",
      "Negative Tweet: @Tatiana_K nope they didn't have it \n",
      "Negative Tweet: @twittera que me muera ? \n",
      "Negative Tweet: spring break in plain city... it's snowing \n",
      "Negative Tweet: I just re-pierced my ears \n",
      "Negative Tweet: @caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\n",
      "Negative Tweet: @octolinz16 It it counts, idk why I did either. you never talk to me anymore \n",
      "Negative Tweet: @smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\n",
      "Negative Tweet: @iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!\n",
      "Negative Tweet: Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\n",
      "Negative Tweet: about to file taxes \n",
      "Negative Tweet: @LettyA ahh ive always wanted to see rent  love the soundtrack!!\n",
      "Negative Tweet: @FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks? \n",
      "Negative Tweet: @alydesigns i was out most of the day so didn't get much done \n",
      "Negative Tweet: one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh* \n",
      "Negative Tweet: @angry_barista I baked you a cake but I ated it \n",
      "Negative Tweet: this week is not going as i had hoped \n",
      "Negative Tweet: blagh class at 8 tomorrow \n",
      "Negative Tweet: I hate when I have to call and wake people up \n",
      "Negative Tweet: Just going to cry myself to sleep after watching Marley and Me.  \n",
      "Negative Tweet: im sad now  Miss.Lilly\n",
      "Negative Tweet: ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again \n",
      "Negative Tweet: Meh... Almost Lover is the exception... this track gets me depressed every time. \n",
      "Negative Tweet: some1 hacked my account on aim  now i have to make a new one\n",
      "Negative Tweet: @alielayus I want to go to promote GEAR AND GROOVE but unfornately no ride there  I may b going to the one in Anaheim in May though\n",
      "Negative Tweet: thought sleeping in was an option tomorrow but realizing that it now is not. evaluations in the morning and work in the afternoon! \n",
      "Negative Tweet: @julieebaby awe i love you too!!!! 1 am here  i miss you\n",
      "Negative Tweet: @HumpNinja I cry my asian eyes to sleep at night \n",
      "Negative Tweet: ok I'm sick and spent an hour sitting in the shower cause I was too sick to stand and held back the puke like a champ. BED now \n",
      "Negative Tweet: @cocomix04 ill tell ya the story later  not a good day and ill be workin for like three more hours...\n",
      "Negative Tweet: @MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge\n",
      "Negative Tweet: @fleurylis I don't either. Its depressing. I don't think I even want to know about the kids in suitcases. \n",
      "Negative Tweet: Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend \n",
      "Negative Tweet: really don't feel like getting up today... but got to study to for tomorrows practical exam... \n",
      "Negative Tweet: He's the reason for the teardrops on my guitar the only one who has enough of me to break my heart \n",
      "Negative Tweet: Sad, sad, sad. I don't know why but I hate this feeling  I wanna sleep and I still can't!\n",
      "Negative Tweet: @JonathanRKnight Awww I soo wish I was there to see you finally comfortable! Im sad that I missed it \n",
      "Negative Tweet: Falling asleep. Just heard about that Tracy girl's body being found. How sad  My heart breaks for that family.\n",
      "Negative Tweet: @Viennah Yay! I'm happy for you with your job! But that also means less time for me and you... \n",
      "Negative Tweet: Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?\n",
      "Negative Tweet: Oh man...was ironing @jeancjumbe's fave top to wear to a meeting. Burnt it \n",
      "Negative Tweet: is strangely sad about LiLo and SamRo breaking up. \n",
      "Negative Tweet: @tea oh! i'm so sorry  i didn't think about that before retweeting.\n",
      "Negative Tweet: Broadband plan 'a massive broken promise' http://tinyurl.com/dcuc33 via www.diigo.com/~tautao Still waiting for broadband we are \n",
      "Positive Tweet: I LOVE @Health4UandPets u guys r the best!! \n",
      "Positive Tweet: im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!\n",
      "Positive Tweet: @DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart. \n",
      "Positive Tweet: Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup\n",
      "Positive Tweet: @LovesBrooklyn2 he has that effect on everyone \n",
      "Positive Tweet: @ProductOfFear You can tell him that I just burst out laughing really loud because of that  Thanks for making me come out of my sulk!\n",
      "Positive Tweet: @r_keith_hill Thans for your response. Ihad already find this answer \n",
      "Positive Tweet: @KeepinUpWKris I am so jealous, hope you had a great time in vegas! how did you like the ACM's?! LOVE YOUR SHOW!! \n",
      "Positive Tweet: @tommcfly ah, congrats mr fletcher for finally joining twitter \n",
      "Positive Tweet: @e4VoIP I RESPONDED  Stupid cat is helping me type. Forgive errors \n",
      "Positive Tweet: crazy day of school. there for 10 hours straiiight. about to watch the hills. @spencerpratt told me too! ha. happy birthday JB! \n",
      "Positive Tweet: @naughtyhaughty HOW DID I FORGET ABOUT TWO AND A HALF MEN?!?!? I LOVE THAT SHOW!!! \n",
      "Positive Tweet: @nileyjileyluver Haha, don't worry! You'll get the hang of it! \n",
      "Positive Tweet: @soundwav2010 At least I won't be the only one feeling lost! This may cause me many later than usual nights, already addicting \n",
      "Positive Tweet: @LutheranLucciol Make sure you DM me if you post a link to that video! &lt;LOL&gt;So I don't miss it   Better get permission and blessing first?\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 Negative Tweets (encoded as 0's)\n",
    "for i,text in enumerate(tweets[tweets['Sentiment'] == 0]['SentimentText']):\n",
    "    print('Negative Tweet:', text)\n",
    "    if i >= 50:\n",
    "        break\n",
    "        \n",
    "# Print first 5 Positive Tweets (encoded as 4's)\n",
    "for i,text in enumerate(tweets[tweets['Sentiment'] == 4]['SentimentText']):\n",
    "    print('Positive Tweet:', text)\n",
    "    if i >= 14:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good idea on using the tokenizer.  we can use this as a function with df.apply to speed this up! Check out the stack overflow solution below for some inspiration.  Some exploratory code is below\n",
    "\n",
    "https://stackoverflow.com/questions/33098040/how-to-use-word-tokenize-in-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "t = TweetTokenizer()\n",
    "\n",
    "\n",
    "def create_tokens(tweet):\n",
    "    try:\n",
    "        tweet = str(tweet.lower())\n",
    "        tokens = t.tokenize(tweet)\n",
    "        tokens = list(filter(lambda x: not x.startswith('@'), tokens)) ##\n",
    "        tokens = list(filter(lambda x: not x.startswith('#'), tokens)) ##\n",
    "        tokens = list(filter(lambda x: not x.startswith('http'), tokens)) ##\n",
    "        return tokens\n",
    "    except:\n",
    "        return \"NC\"\n",
    "\n",
    "tweets['SentimentTextTokenized'] = tweets['SentimentText'].apply(create_tokens)\n",
    "tweets = tweets[tweets.SentimentTextTokenized != 'NC']\n",
    "tweets.reset_index(inplace=True)\n",
    "tweets.drop('index', inplace=True, axis=1)\n",
    "tweets.head()\n",
    "\n",
    "\n",
    "X = tweets.SentimentTextTokenized\n",
    "Y = tweets.Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+yTPVD38AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20cd37b4d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "num_bins = 5\n",
    "n, bins, patches = plt.hist(tweets.Sentiment, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Looks like there are only two categories in this Training Dataset. Negative = 0, Positive = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & (future) Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338461     [if, u, came, to, visit, here, in, 1779, it, w...\n",
      "986385     [thanks, !, good, advice, for, this, day, \", s...\n",
      "1301957    [thank, you, ,, roger, !, oh, ,, and, very, ni...\n",
      "1003165    [hi, matt, ,, how, are, you, today, ?, i, am, ...\n",
      "1369722                       [that's, cool, ,, i, like, it]\n",
      "1037256    [hi, hi, hi, !, !, !, ::, waves, frantically, ...\n",
      "443905       [has, given, up, with, this, king, lear, shite]\n",
      "119387     [i, want, all, my, friends, with, me, now, mis...\n",
      "701129                                   [i, have, to, work]\n",
      "1143523         [-, you, have, sexy, eyes, even, drunk, lol]\n",
      "Name: SentimentTextTokenized, dtype: object\n",
      "338461     0\n",
      "986385     4\n",
      "1301957    4\n",
      "1003165    4\n",
      "1369722    4\n",
      "1037256    4\n",
      "443905     0\n",
      "119387     0\n",
      "701129     0\n",
      "1143523    4\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.20,random_state=100)\n",
    "print(X_train[:10])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13644450"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert each word into a vector representation. Couldn't get Keras working with straight indexes for each word so I followed the steps laid out here:\n",
    "# https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html\n",
    "\n",
    "vec_dim = 100\n",
    "\n",
    "tweet_w2v = Word2Vec(size=vec_dim, min_count=10) #vector size and minimum threshold to include for rare words\n",
    "tweet_w2v.build_vocab(x for x in X_train)\n",
    "tweet_w2v.train((x for x in X_train), total_examples=tweet_w2v.corpus_count, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.66520756483078),\n",
       " ('horrible', 0.6610486507415771),\n",
       " ('good', 0.6542518138885498),\n",
       " ('weird', 0.6290618181228638),\n",
       " ('painful', 0.5871548652648926),\n",
       " ('badly', 0.5763333439826965),\n",
       " ('lame', 0.5693964958190918),\n",
       " ('sad', 0.5642663240432739),\n",
       " ('awful', 0.5640964508056641),\n",
       " ('rough', 0.5562361478805542)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Vectors \n",
    "https://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 30903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, vec_dim) for z in map(lambda x: x, X_train)])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_vecs_w2v)\n",
    "train_vecs_w2v = scaler.transform(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, vec_dim) for z in map(lambda x: x, X_test)])\n",
    "test_vecs_w2v = scaler.transform(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Y: 338461     0\n",
      "986385     4\n",
      "1301957    4\n",
      "1003165    4\n",
      "1369722    4\n",
      "1037256    4\n",
      "443905     0\n",
      "119387     0\n",
      "701129     0\n",
      "1143523    4\n",
      "Name: Sentiment, dtype: int64\n",
      "Encoded Y: [0 1 1 1 1 1 0 0 0 1]\n",
      "One Hot Y: [[ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# This section only needed if doing more than binary classification (multi-class)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Original Y:\", y_train[:10])\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test= encoder.transform(y_test)\n",
    "print(\"Encoded Y:\", y_train[:10])\n",
    "\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "print(\"One Hot Y:\", y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model (really simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      " - 27s - loss: 0.4844 - acc: 0.7662\n",
      "Epoch 2/9\n",
      " - 26s - loss: 0.4745 - acc: 0.7727\n",
      "Epoch 3/9\n",
      " - 26s - loss: 0.4720 - acc: 0.7742\n",
      "Epoch 4/9\n",
      " - 26s - loss: 0.4706 - acc: 0.7750\n",
      "Epoch 5/9\n",
      " - 26s - loss: 0.4696 - acc: 0.7756\n",
      "Epoch 6/9\n",
      " - 26s - loss: 0.4688 - acc: 0.7760\n",
      "Epoch 7/9\n",
      " - 26s - loss: 0.4684 - acc: 0.7764\n",
      "Epoch 8/9\n",
      " - 26s - loss: 0.4680 - acc: 0.7763\n",
      "Epoch 9/9\n",
      " - 26s - loss: 0.4677 - acc: 0.7767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d0bf06908>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the sequence to the same length (this hurt accuracy?)\n",
    "# train_vecs_w2v = sequence.pad_sequences(train_vecs_w2v, maxlen=vec_dim)\n",
    "# test_vecs_w2v = sequence.pad_sequences(test_vecs_w2v, maxlen=vec_dim)\n",
    "\n",
    "# Build Keras Model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=vec_dim))\n",
    "model.add(Dense(2, activation='softmax')) # softmax if multi-class\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy', # categorical_crossentropy if multi-class\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=3, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.76%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to Predict Positive/Neutral/Negative\n",
    "\n",
    "def prediction(text):\n",
    "    sentiment = [\"Negative\", \"Positive\"]\n",
    "    text = text.split() # Tokenize\n",
    "    text = buildWordVector(text, vec_dim)\n",
    "    text = scaler.transform(text)\n",
    "    predic = model.predict(text, batch_size=32)\n",
    "    result = sentiment[predic.argmax(axis=1)[0]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 40ft Japanese boat found near Hug Point!, ups: 96, downs: 0, Have we visited: False\n",
      "--------------------\n",
      "Parent ID: 7i1qn8\n",
      "Comment ID: dqvk7mw\n",
      "Imagine surfing and having this thing floating half submerged in the lineup.  Scary stuff.  \n",
      "\n",
      "Oh and mirelurks.  \n",
      "########## PREDICTED SENTIMENT: Negative ##########\n",
      "--------------------\n",
      "Parent ID: 7i1qn8\n",
      "Comment ID: dqvg5w5\n",
      "[might need reported](https://marinedebris.noaa.gov/current-efforts/emergency-response/japan-tsunami-marine-debris)\n",
      "########## PREDICTED SENTIMENT: Negative ##########\n",
      "--------------------\n",
      "Parent ID: 7i1qn8\n",
      "Comment ID: dqvllew\n",
      "Where's the banana?\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: 7i1qn8\n",
      "Comment ID: dqvhp3r\n",
      "The invasion has started!\n",
      "########## PREDICTED SENTIMENT: Negative ##########\n",
      "--------------------\n",
      "Parent ID: 7i1qn8\n",
      "Comment ID: dqvugl6\n",
      "[Here is an album](https://imgur.com/a/7vfO4) of more photo's a friend at the Seaside Aquarium took. The name of the ship is  第一政丸 (Dai ichi Masa-maru). Dai ichi  means No.1, Masa might be a company name. \n",
      "\n",
      "The boat was covered in large, pelagic gooseneck barnacles which indicated it had been floating out at sea for quite some time because pelagic gooseneck barnacles are a species of barnacles that only attached to drifting debris (you'll will often see them attached to driftwood). The boat has been examined and is not believed to have any non-native species. Plans are underway to remove it from the beach.\n",
      "\n",
      "It's still at the south end of Arcadia Beach if you want to see it!\n",
      "\n",
      "########## PREDICTED SENTIMENT: Negative ##########\n",
      "--------------------\n",
      "Parent ID: dqvk7mw\n",
      "Comment ID: dqvmoul\n",
      "Ugh that's a terrible thought.\n",
      "########## PREDICTED SENTIMENT: Negative ##########\n",
      "--------------------\n",
      "Parent ID: dqvk7mw\n",
      "Comment ID: dqvozfv\n",
      "or in a sailboat race from San Francisco to Hawaii and having it rip off your keel or rudder.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvg5w5\n",
      "Comment ID: dqvgdt3\n",
      "Thanks for that! Just sent NOAA an email, I feel really cool now!\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvg5w5\n",
      "Comment ID: dqvgdk5\n",
      "nuka-boat!\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvhp3r\n",
      "Comment ID: dqvr7ok\n",
      "They've been patiently waiting 72 years for the right time to strike\n",
      "########## PREDICTED SENTIMENT: Negative ##########\n",
      "--------------------\n",
      "Parent ID: dqvhp3r\n",
      "Comment ID: dqvu8fb\n",
      "It was definitely sailed here by some sort of crazy nuclear tsunami mutant...we’re all screwed \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvgdt3\n",
      "Comment ID: dqvms73\n",
      "That boats been there for days. [Heres an article from three days ago](http://www.dailyastorian.com/Local_News/20171204/capsized-boat-washes-ashore-between-hug-point-and-acardia-beach). That’s cute you thought you were the first to discover it though. Kindve surprised no ones out signs up for it though. Usually they get roped off.\n",
      "########## PREDICTED SENTIMENT: Negative ##########\n",
      "--------------------\n",
      "Parent ID: dqvgdk5\n",
      "Comment ID: dqvggpp\n",
      "Probably a lot of mutant starfish under that thing. Cannon beach will be overrun by the end of the week.\n",
      "########## PREDICTED SENTIMENT: Negative ##########\n",
      "--------------------\n",
      "Parent ID: dqvms73\n",
      "Comment ID: dqvn0pn\n",
      "> That’s cute you thought you were the first to discover it though.\n",
      "\n",
      "Condescending much? It's right near Hug Point, why would you assume I thought I was the first person to discover it? I hadn't seen anything about it posted here, so I thought I'd share it.\n",
      "\n",
      "Edit: I guess I kind of implied that I thought I was the first person to see it, but I just meant I felt cool sending an email to NOAA.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvms73\n",
      "Comment ID: dqvt86s\n",
      "have fun in downvote oblivion \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvggpp\n",
      "Comment ID: dqvgtq7\n",
      "My bet is on mirelurks.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvn0pn\n",
      "Comment ID: dqvnqbs\n",
      "Jesus tap dancing Christ. Can r/portland just let someone post a picture of a boat without being ass holes? \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvn0pn\n",
      "Comment ID: dqvngdk\n",
      "Be honest with yourself - you had a big ol' boner to run home and post it up for some sweet, sweet internet points.  \n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvgtq7\n",
      "Comment ID: dqvicos\n",
      "they's folk sell good cakes down by the dock\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvgtq7\n",
      "Comment ID: dqvkym9\n",
      "🤣\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvnqbs\n",
      "Comment ID: dqvo2mc\n",
      "Haha it's all good brother, I'm fine admitting that I was excited to post it here and hoped it would be popular.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvngdk\n",
      "Comment ID: dqvnwam\n",
      "Man this sub has been cynical as fuck lately.\n",
      "\n",
      "Shaming people for posting shit is kinda counter productive to the purpose of the sub and reddit as a whole right? Or would you rather r/Portland just be posts about Californians, traffic, and hating homeless people?\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvngdk\n",
      "Comment ID: dqvt7r8\n",
      "here's some downvotes for you sir\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n",
      "--------------------\n",
      "Parent ID: dqvnwam\n",
      "Comment ID: dqvpeo1\n",
      "This guy sounds like he got his moms dildo stuck up his ass and is trying to figure out how to tell her. 99% of us thought it was a good post.\n",
      "########## PREDICTED SENTIMENT: Positive ##########\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "#this is a read-only instance\n",
    "reddit = praw.Reddit(user_agent='first_scrape (by /u/dswald)',\n",
    "                     client_id='TyAK1zSuAvQjmA', \n",
    "                     client_secret=\"uxHGsL0zNODbowN6umVnBWpqLAQ\")\n",
    "\n",
    "subreddit = reddit.subreddit('portland')\n",
    "hot_python = subreddit.hot(limit = 3) #need to view >2 to get past promoted posts\n",
    "\n",
    "for submission in hot_python:\n",
    "    if not submission.stickied: #top 2 are promoted posts, labeled as 'stickied'\n",
    "        print('Title: {}, ups: {}, downs: {}, Have we visited: {}'.format(submission.title,\n",
    "                                                                          submission.ups,\n",
    "                                                                          submission.downs,\n",
    "                                                                          submission.visited))\n",
    "        comments = submission.comments.list() #unstructured\n",
    "        for comment in comments:\n",
    "            print (20*'-')\n",
    "            print ('Parent ID:', comment.parent())\n",
    "            print ('Comment ID:', comment.id)\n",
    "            print (comment.body)\n",
    "            print(\"#\"*10,'PREDICTED SENTIMENT:', prediction(comment.body),\"#\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that everythng is positive or neutral.  Is this because of a unigram model or too specific training data or what?  This is food for thought.  Committing now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
